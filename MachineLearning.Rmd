---
title: "Machine learning log"
author: "Larissa Bouwknegt"
date: "2023-10-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

When loading the data in Weka a filter (filters -> unsupervised -> NumericToNominal) needs to be applied to the age atribute to make sure the age atribute is nominal instead of numeric. Saving as an arff file prevents us to have to repeat this action every time. The most common algorithms will be run with 10 fold cross validation  and be added to a table to see their individual performances without changed parameters. 

```{r}
algorithm_scores <- data.frame(algorithm = c("ZeroR", "OneR"),
                               Percentage_Correct = c(64.9781, 73.9779),
                               Percentage_Wrong = c(35.0219, 26.0221)
                               )
naive_bayes <- c("Naive Bayes", 68.1409, 31.8591)
algorithm_scores <- rbind(algorithm_scores, naive_bayes)
j48 <- c("j48", 75.495, 24.505)
algorithm_scores <- rbind(algorithm_scores, j48)
IBk <- c("IBk", 70.5837, 29.4163)
algorithm_scores <- rbind(algorithm_scores, IBk)
logistic <- c("Logistic", 77.7064, 22.2936)
algorithm_scores <- rbind(algorithm_scores, logistic)
suport_vector_machine <- c("SMO", 76.4464, 23.5536)
algorithm_scores <- rbind(algorithm_scores, suport_vector_machine)
qda <- c("QDA (sex atribute removed)", 75.7521, 24.2479)
algorithm_scores <- rbind(algorithm_scores, qda)
bagging <- c("Bagging", 77.7578, 22.2422)
algorithm_scores <- rbind(algorithm_scores, bagging)
voting <- c("Vote", 64.9781, 35.0219)
algorithm_scores <- rbind(algorithm_scores, voting)
adaboost <- c("AdaBoostM1", 72.1265, 27.8735)
algorithm_scores <- rbind(algorithm_scores, adaboost)
stacking <- c("Stacking", 64.9781, 35.0219)
algorithm_scores <- rbind(algorithm_scores, stacking)
random_forest <- c("Random Forest", 77.9121, 22.0879)
algorithm_scores <- rbind(algorithm_scores, random_forest)
clustering_class <- c("ClassificationViaClustering", 62.4839, 37.5161)
algorithm_scores <- rbind(algorithm_scores, clustering_class)
knitr::kable(algorithm_scores[order(algorithm_scores$Percentage_Correct, decreasing = TRUE),], row.names = F)
```

The table above is sorted on the best scoring algorithms without any parameters changed except for the QDA where the gender of the crabs was removed. All the tests were done with 10 fold cross validation. 3 algorithms scored even lower then or equal to zeroR which is interesting considering that it just takes the most prominent value. When further investigating the exact 2 scores same scores of voting and stacking to ZeroR the conclusion fell that we needed to specify the algorithms there. So these 2 were rerun with 11 algorithms, all of the above except for voting and stacking self and QDA was also not included due to the fact that we want to use the sex attribute. 

```{r}
rerun <- data.frame(algorithm = c("Voting", 
                                  "Stacking with j48 meta", 
                                  "stacking with random forest meta"),
                    Percentage_Correct = c(76.6264, 76.4207, 78.272),
                    Percentage_Wrong = c(23.3736, 23.5793, 21.728))

knitr::kable(rerun)
```

The results gained from voting and stacking seems to be much better now. The meta classifier used for stacking was a j48 tree and after that a run with a random forest meta learner. The best scoring algorithms are: Stacking with a random forest, Random forest, bagging, logistic and voting. For now we will look further into these 5 algorithms and see if we can optimize the parameters to prevent over fitting and get better results. 

Before tweaking the algorithms let's see if the dataset can be tweaked for optimal performace. In weka in the select atributes tab the CfsSubsetEval with exhaustive search forward and backward and no different changes (2 different runs) both give Selected attributes: 1,3,4,7,8 
                     Sex,
                     Diameter,
                     Height,
                     Viscera.Weight,
                     Shell.Weight.
A gainratio select atributes with the ranker search method gives the following results: 
 0.0813   8 Shell.Weight
 0.0726   7 Viscera.Weight
 0.0721   3 Diameter
 0.0674   5 Weight
 0.066    2 Length
 0.0629   4 Height
 0.0579   6 Shucked.Weight
 0.0574   1 Sex
Lets see if these attributes are also selected for the individual algorithms when WrapperSubsetEval is run for each one with BestFirst and backwards as option within BestFirst. Since Random forest are build based on Random trees we will use the random tree here for a shorter calculation time. For the reason of time consumption voting and stacking were also not run through this. 

```{r}
selected_results <- data.frame(algorithm = c("Random Tree",
                                             "Logistic",
                                             "Bagging"),
                               selected_atributes = c("Sex, Length,
                               Weight, Shucked.Weight,
                               Shell.Weight", 
                               "Weight, Shucked.Weight, Shell.Weigh", 
                               "Sex, Weight, Shucked.Weight,
                               Shell.Weight"))
knitr::kable(selected_results, format = "simple")
```
4 sub sets are made, based on the 3 algorithms and the cfssubseteval. These are then tested in the explorer against each other.

The following are the results of a T test where the different datasets are tested with the percentage correct for the algorithms. This shows that the 4th smaller dataset scores significantly worse in all the algorithms and the 2nd and 3rd cleaned datasets when it comes to a random forest. 
![](Ttestswapresults.png)

The are under the curve shows us that the area under the curve is even significantly worse with all the smaller datasets for 2 different algorithms. The last smaller dataset is  worse for all algorithms. 
![](TtestswapresultsAOC.png)

So lets focus on how the 5 algorithms scored compared to the whole dataset. Since stacking has the highest score this algorithm is used as the comparison. 

![](Stacking.png)

![](StackingAUC.png)

Vote scores significantly worse then stacking. When looking at the area under the curve stacking scores worse as well on the original dataset. 

So lets see if we can tweak the 4 algorithms without vote a bit more on the original dataset. 

Let's start with the random forest, since it is build on random trees we will try to improve an individual tree before 

```{r}
tree_scores <- data.frame(tree_depth = c("unlimited",
                                         "5",
                                         "6",
                                         "7",
                                         "8"),
                          percentage_corect = c(70.6351,
                                                74.1322,
                                                75.0064,
                                                75.6236,
                                                74.775))

knitr::kable(tree_scores, format = "simple")
```
The table shows the changes to the depth of the tree parameter and how they corresponded to the percentage correct with 10 fold cross validation. Now lets build a forest with trees with a depth of 7 since this seems to be the best parameter score wise. A forest with the depth of 7 gives a 78.5035 percent accuracy. By increasing the trees used under the iteration value to a 1000 instead of the standard 100 we get a 78.7349 percent accuracy. Increasin this to 10000 does take a lot more time and does not improve the score at all. 

By bagging we can change the number of iterations:

```{r}
bagging_itter <- data.frame(itterations = c("10",
                                            "100",
                                            "1000"),
                            percentage_correct = c(77.7578,
                                                   78.2206,
                                                   78.1435))

knitr::kable(bagging_itter, format = "simple")
```
Changing the number of iterations improves it when it is set to a 100 but decreases when set to a 1000. A 1000 iterations also takes more time and it is getting on the long side there with a time of 20.8 to build the model. SO bagging seems to be optimal at a 100 iterations for now. 
